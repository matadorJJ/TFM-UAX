# -*- coding: utf-8 -*-
"""GNN_main_final_Histogramas.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1dbOCJzwlFM8ZLIW5dOIbfK5deKVDeT
"""

#
# Juan Julián Moreno Piedra
# 27-8-2024
#
#
#

!pip install torch_geometric

#
# Importing Libraries
#

# OS y Google
import os
import io
import gdown

# Pandas, Numpy, Matplotlib, Seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.sparse import coo_matrix

# Import x
import networkx as nx
from networkx.convert_matrix import to_numpy_array

# Pytorch
import torch
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv
from torch_geometric.utils import from_networkx
import torch.nn as nn

# Tensorflow
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dropout
from tensorflow.keras.optimizers import Adam

# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

def normalize_numeric_features(df):
  """
  Normalizes the numerical features of a DataFrame to a range between 0 and 1 using Min-Max scaling.

  Args:
      df: The Pandas DataFrame containing the features.

  Returns:
      A new DataFrame with the normalized numerical features.
  """

  # Select only the numerical columns
  numeric_cols = df.select_dtypes(include='number').columns

  # Apply Min-Max scaling to the numerical columns
  df[numeric_cols] = (df[numeric_cols] - df[numeric_cols].min()) / (df[numeric_cols].max() - df[numeric_cols].min())

  return df

def plot_histograms(df):
    # Seleccionar solo las columnas numéricas
    df_numeric = df.select_dtypes(include='number')

    # Definir el número de subplots basado en la cantidad de columnas numéricas
    num_columns = len(df_numeric.columns)
    fig, axes = plt.subplots(nrows=(num_columns // 3) + 1, ncols=3, figsize=(15, 5 * ((num_columns // 3) + 1)))

    # Aplanar los ejes para iterar fácilmente (si hay más de 1 fila de subplots)
    axes = axes.flatten()

    # Crear un histograma para cada columna numérica
    for i, column in enumerate(df_numeric.columns):
        sns.histplot(df_numeric[column], kde=True, bins=15, ax=axes[i])
        axes[i].set_title(f'Histograma de {column}')
        axes[i].set_xlabel(column)
        axes[i].set_ylabel('Frecuencia')

    # Eliminar cualquier subplot vacío (si el número de columnas no es múltiplo de 3)
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    # Ajustar el layout
    plt.tight_layout()
    plt.show()

# downoading CST data file from Google Drive
# file has been modified to mask the real name of the servers
# to guarantee confienciality
#
file_id = '13FQTB0WPCf9gvzwegPBIgBaOyr_RrmoO'
# file_id= '138rPG6moGxMKkt77ogCoCRdBTE_Eh6Ni'
url = f"https://drive.google.com/uc?id={file_id}"
output = 'se.tab'  # Nombre del archivo de salida
gdown.download(url, output, quiet=False)

# downoading Sessions data file from Google Drive
# file has been modified to mask the real name of the servers
# to guarantee confienciality
#
file_id='13EyDH2cV4RhKsackbpIxO7zWL3fjS5vr'
# file_id = '14uyoXlGowfDhE7Eoh5DqdTPwnROnGkGV'
# file_id= '138rPG6moGxMKkt77ogCoCRdBTE_Eh6Ni'
url = f"https://drive.google.com/uc?id={file_id}"
output = 'archivo.csv'  # Nombre del archivo de salida
gdown.download(url, output, quiet=False)

# Reading the Backup Sessions CSV file
df_backup = pd.read_csv('archivo.csv')

# Import the CSV file
df_cst = pd.read_csv('se.tab')

# Feature cleaning in df_cst

df_cst.info()

# Renaming '# Client' to 'Client' in df_cst
df_cst = df_cst.rename(columns={'# Client': 'Client'})
df_cst=df_cst.rename(columns={'# Files': 'Total # Files'})

# Deleting'# Running DA' and '# Pending DA' in df_cst
# This colums contains info from backups in use witch
# is not the case
df_cst = df_cst.drop(columns=['# Running DA', '# Pending DA'])


# Doublecheck
print(df_cst.head())

# Eliminar el símbolo de porcentaje y convertir a decimal
df_cst['Success'] = df_cst['Success'].str.rstrip('%').astype('float') / 100

# Verificar el cambio
print(df_cst['Success'].head())

#
# Feature cleaning df_backup
#

df_backup.info()

# There are some nulls values and also some redundant files,
# as some of the date fields are in both Unix timestamp and
# estandar date format

# Let's check where are the null values
df_backup.isnull().sum()

# There are nulls in the Device and Protection fields.

# In the protection field it is normal, since it is not mandatory to specify the
# protection

# Some data is missing, in some cases there is no information about the Device where the backup was made.
# most likely in cases where an error has occurred in the backup before starting the writing process,
# for whatever reason, either because the device was not available or because there was an
# incident at the source

# Checking the Protection field first
df_backup[df_backup['Protection'].isnull()].head(10)

# I am going to analyze a rows with null Protection, in principle
# does not affect us too much, since they are simply backup jobs that do not expire
# and therefore have no protection, that is, they are alive in the database of the
# data protector database forever. Possibly most of them are backups of the
# IDB, which is Data Protector's own database, or backups of the business-critical
# applications, in any case I will check it out.

# Fitering files with Protection == Null
rows_without_protection = df_backup[df_backup['Protection'].isnull()]

# Counting unique values in the column Description
description_counts = rows_without_protection['Description'].value_counts()

# Showing results
print(description_counts)

# As spected, the data corroborate the predictions, they are therefore rows with
# backups of the IDB (DP database), critical applications such as MSSQL or
# Oracle and in some cases are simply errors where
# protection has not been specified.

# To make the rows usable I'll replace the null values
# with a date in the past, such as 1/1/1900 12:00:00 AM

df_backup.fillna({'Protection': '1/1/1900 12:00:00 AM'}, inplace=True)

df_backup.isnull().sum()

# Similar approach to thouse files with Protection = Permanent, but in this case
# I'll use a date in the future
df_backup['Protection'] = df_backup['Protection'].replace('Permanent', '1/1/2200 12:00:00 AM')

# Let's analalize the case  of nulls in Device, it must be a situation where either
# an error occurred in the backup at the time of reading the data at the
# source or the target devices were not available at the time of
# the backup and therefore the backup failed. It could also be cases where
# the backup execution was aborted before making the copy to the # device.
# device

# Fitering files where Device is null
rows_without_device = df_backup[df_backup['Device'].isnull()]

# Counting unique values for Status
status_counts = rows_without_device['Status'].value_counts()

# Showing results
print(status_counts)

# As expected, sessions failed or aborted, but in this case as it's a reduced
# number I'm going to delete the rows

df_backup.dropna(subset=['Device'], inplace=True)

df_backup.isnull().sum()

# Converting dates from Objt to Date Format

# This is the new format to avoid the warning, I'm not using it becasue in some cases
# it can trigger an error
#df_backup['Start Time'] = pd.to_datetime(df_backup['Start Time'], format='%Y-%m-%d %H:%M:%S')
#df_backup['End Time'] = pd.to_datetime(df_backup['End Time'], format='%Y-%m-%d %H:%M:%S')
#df_backup['Protection'] = pd.to_datetime(df_backup['Protection'], format='%Y-%m-%d %H:%M:%S')

df_backup['Start Time'] = pd.to_datetime(df_backup['Start Time'])
df_backup['End Time'] = pd.to_datetime(df_backup['End Time'])
df_backup['Protection'] = pd.to_datetime(df_backup['Protection'])

# visual check
df_backup.head(15)

# Now I'm going to convert the column Duration to segs, so I can use it
# in the models

# Checking 'Duration [hh:mm]' is a string
df_backup['Duration [hh:mm]'] = df_backup['Duration [hh:mm]'].astype(str)

# Converting to second
def convert_to_seconds(duration):
    hh, mm = map(int, duration.split(':'))
    return hh * 3600 + mm * 60

# Updating the dataframe
df_backup['Duration [hh:mm]'] = df_backup['Duration [hh:mm]'].apply(convert_to_seconds)

# Changing name from 'Duration [hh:mm]' to 'Duration_seconds'
df_backup.rename(columns={'Duration [hh:mm]': 'Duration_seconds'}, inplace=True)

# Verifying
print(df_backup[['Duration_seconds']].head())

# Converting to categorical variables
#
df_backup['Mode'] = df_backup['Mode'].astype('category')
df_backup['Status'] = df_backup['Status'].astype('category')
df_backup['Client'] = df_backup['Client'].astype('category')
df_backup['Device'] = df_backup['Device'].astype('category')
df_backup['# Object Type'] = df_backup['# Object Type'].astype('category')

# Checking the number of categories
num_categories = df_backup['Client'].nunique()
print(f"Number of categories in 'Client': {num_categories}")

num_categories = df_backup['Device'].nunique()
print(f"Number of categories in 'Device': {num_categories}")

num_categories = df_backup['Status'].nunique()
print(f"Number of categories in 'Status': {num_categories}")

num_categories = df_backup['Mode'].nunique()
print(f"Number of categories in 'Mode': {num_categories}")

num_categories = df_backup['# Object Type'].nunique()
print(f"Number of categories in '# Object Type': {num_categories}")

df_backup.info()

#
# The pure fact of using the power fo Python brings the DP customers
# the capability of create custom reportings usign the imported data
# Let me create a few examples to demonstrate one of the objectives
# of the TFM
#

# Performance mean by client
performance_by_client = df_backup.groupby('Client', observed = True)['Performance [MB/min]' ].mean()
print(performance_by_client)

# Performance by client in a graphic mode
plt.figure(figsize=(24, 6))
performance_by_client.plot(kind='bar', color='salmon')
plt.title('Performance by Client')
plt.xlabel('Device')
plt.ylabel('Performance [MB/min]')
plt.grid(True)
plt.show()

# Performance by device
#
# Calculating mean
performance_by_device = df_backup.groupby('Device', observed = True)['Performance [MB/min]' ].mean()

# Plotting
plt.figure(figsize=(10, 6))
performance_by_device.plot(kind='bar', color='green')
plt.title('Performance by device')
plt.xlabel('Device')
plt.ylabel('Performance [MB/min]')
plt.grid(True)
plt.show()

# Performance by client sorted
plt.figure(figsize=(48, 48))
performance_by_client.sort_values(ascending=False).plot(kind='barh', color='skyblue')  # Horizontal bar chart
plt.title('Rendimiento por Cliente')
plt.xlabel('Rendimiento [MB/min]')
plt.ylabel('Cliente')
plt.grid(True)
plt.show()

#
# Top ten customers and device with more errors
#
# Function to calculate
def is_error(row):
    # In case we liked to add warnings we will use this fuction
    #
    return row['# Errors'] > 0

# Filter by error
error_backups = df_backup[df_backup.apply(is_error, axis=1)]

# Gruping by client and calculating the total
errors_by_client = error_backups.groupby('Client', observed=True)['# Errors'].sum()

# Gruping by device and calculating the total
errors_by_device = error_backups.groupby('Device', observed=True)['# Errors'].sum()

# Ploting the bars using a subplot
plt.figure(figsize=(12, 6))

# Clients
plt.subplot(1, 2, 1)
errors_by_client.head(10).plot(kind='bar')  # Showing the top 10
plt.xlabel('Cliente')
plt.ylabel('Número de errores')
plt.title('Top 10 clientes con más errores')

# Device
plt.subplot(1, 2, 2)
errors_by_device.head(10).plot(kind='bar')  # Showing the top 10
plt.xlabel('Dispositivo')
plt.ylabel('Número de errores')
plt.title('Top 10 dispositivos con más errores')

plt.tight_layout()
plt.show()

#
# Size of backup by time
#

# Grouping and calculating the mean
df_backup_grouped = df_backup.groupby('Start Time')['Size [kB]'].mean()

# Ploting
plt.figure(figsize=(24, 6))
plt.plot(df_backup_grouped.index, df_backup_grouped.values)
plt.xlabel('Date')
plt.ylabel('Backup average (MB)')
plt.title('Evolution of backup size over time')
plt.grid(True)
plt.show()

#
# Grouping by month so it's more readible
#
# Using data from 2015 and Status = "Full"
full_backups_since_2015 = df_backup[(df_backup['Start Time'] >= ('2015-01-01')) & (df_backup['Mode'] == 'full')]

# Grouping and calculating average
yearly_avg_since_2015 = full_backups_since_2015.groupby(pd.Grouper(key='Start Time', freq='Y'))['Size [kB]'].mean()

# Plotting
plt.figure(figsize=(24, 12))
sns.barplot(x=yearly_avg_since_2015.index, y=yearly_avg_since_2015.values)
plt.xlabel('Año')
plt.ylabel('Tamaño medio de backups completos (kB)')
plt.title('Tamaño medio anuall de backups completos desde 2015')
plt.xticks(rotation=45)
plt.show()

#
# Now let's see some reports we can pull using Graphs
#

# Creating a Multi Directed Graph using Network x
G = nx.MultiDiGraph()

# Preparing information for nodes, using Clients and Devices
clients = df_backup['Client'].unique()
devices = df_backup['Device'].unique()

# Adding nodes to the Graph
G.add_nodes_from(clients, node_type='Client')
G.add_nodes_from(devices, node_type='Device')

# Adding edges, including some charactesitics
for _, row in df_backup.iterrows():
    client = row['Client']
    device = row['Device']

    # Adding edgest to the graph
    G.add_edge(client, device,
               status=row['Status'],
               mode=row['Mode'],
               duration_seconds=row['Duration_seconds'],
               size=row['Size [kB]'],
               performance=row['Performance [MB/min]'],
               start=row['Start Time_t'],
               end=row['End Time_t'],
               mountpoint=row['Object Name']
               )

#
# Top ten Clients with more backups
#

# Counting the number of outgoing and incomming edges of each node (client).
in_degrees = dict(G.in_degree())
out_degrees = dict(G.out_degree())

# Getting the top Clients
top_clients = sorted(out_degrees.items(), key=lambda item: item[1], reverse=True)[:10]

# Plotting
plt.bar([client for client, count in top_clients], [count for client, count in top_clients])
plt.xlabel('Cliente')
plt.ylabel('Número de copias de seguridad')
plt.xticks(rotation=90)
plt.title('Top 10 clientes con más copias de seguridad')
plt.show()

#
# Top 10 devices more used
#

# Counting the number of outgoing and incomming edges of each node.
in_degrees = dict(G.in_degree())
out_degrees = dict(G.out_degree())

# For this particular case we will use the incoming edges
# as we are looking for devices (in_degrees)
top_devices = sorted(in_degrees.items(), key=lambda item: item[1], reverse=True)

# filtering by the top 10
num_top_devices = 10
top_devices = top_devices[:num_top_devices]

# Plotting
plt.bar([device for device, count in top_devices], [count for device, count in top_devices])
plt.xlabel('Dispositivo')
plt.ylabel('Número de copias de seguridad')
plt.xticks(rotation=90)  # Ajusta la rotación si los nombres son largos
plt.title(f'Top {num_top_devices} dispositivos más utilizados')
plt.show()

#
# Top 10 clientes by backup frecuency
#

# 1. Create a dictionary to count the backup frequency for each client
backup_frequency = {}
for client, _, _ in G.edges(keys=True):
    backup_frequency[client] = backup_frequency.get(client, 0) + 1

# 2. Sort the clients based on their backup frequency in descending order
sorted_backup_frequency = sorted(backup_frequency.items(), key=lambda item: item[1], reverse=True)

# 3. Get the top 10 clients
top_10_clients = sorted_backup_frequency[:10]

# Plotting
plt.figure(figsize=(10, 6))
plt.bar([client for client, count in top_10_clients], [count for client, count in top_10_clients])
plt.xlabel('Client')
plt.ylabel('Backup Frequency')
plt.title('Top 10 Clients by Backup Frequency')
plt.xticks(rotation=90)
plt.show()

# Calculate graph density
density = nx.density(G)

# Print the density value
print(f"The graph density is: {density:.4f}")

# Get weakly connected components for a directed graph
connected_components = list(nx.weakly_connected_components(G))

# Count the number of nodes in each component
component_sizes = [len(component) for component in connected_components]

# Plotting
plt.figure(figsize=(8, 5))  # Adjust figure size if needed
plt.bar(range(1, len(component_sizes) + 1), component_sizes)
plt.xlabel('Component Number')
plt.ylabel('Number of Nodes')
plt.title('Weakly Connected Components') # Changed title to reflect change in function
plt.show()

# Print the number of connected components
print(f"Number of weakly connected components: {len(connected_components)}") # Changed text to reflect change in function

# Interesting, so all the clients but 1 have at least 2 backups to two different devies

#
# Top 10 clients by Degree Centrality
#

# Calculate degree centrality
degree_centrality = nx.degree_centrality(G)
# Calculate betweenness centrality
betweenness_centrality = nx.betweenness_centrality(G)

# Sort nodes by centrality (descending order)
sorted_degree_centrality = sorted(degree_centrality.items(), key=lambda item: item[1], reverse=True)
sorted_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda item: item[1], reverse=True)

# Get the top 10 nodes for each centrality measure
top_10_degree_nodes = sorted_degree_centrality[:10]
top_10_betweenness_nodes = sorted_betweenness_centrality[:10]

# Plotting degree centrality
plt.figure(figsize=(10, 6))
plt.bar([node for node, centrality in top_10_degree_nodes], [centrality for node, centrality in top_10_degree_nodes])
plt.xlabel('Node')
plt.ylabel('Degree Centrality')
plt.title('Top 10 Nodes by Degree Centrality')
plt.xticks(rotation=90)
plt.show()

#
# Starting the use case, in order to have a point of recovery
# I'll be using a copy of the dataset
df=df_backup.copy()

#
# Adding SLA information to the nodes, this information have to be entered
# manualy by the customers, but in the case of this exercice I'm going to
# do it in an automated way.
#

# Sort the dataframe in descending order by `Performance [MB/min]`
df = df.sort_values(by='Performance [MB/min]', ascending=False)

# Create new columns
df['RPO'] = 0
df['RTO'] = 0


# Define values and counts
rpo_values = [24, 48, 72]
rto_values = [12, 48, 72]

counts = [10, 10, len(df) - 20]

# Assign values directly based on sorted order (no shuffling)
start = 0
for rpo, rto, count in zip(rpo_values, rto_values, counts):
    df.iloc[start:start + count, df.columns.get_loc('RPO')] = rpo
    df.iloc[start:start + count, df.columns.get_loc('RTO')] = rto

    start += count

# Sort the dataframe in ascending order by `Start Time_t`
df = df.sort_values(by='Start Time_t')

# Print the first 30 rows of the updated dataframe
print(df.head())

# Merging the two dataframes to have a single repository to build the graph

# Merging using client as index
df = pd.merge(df, df_cst, on='Client', how='left')

# check
print(df.head())

#
# Adding the maximum performance for the device, this information can be used
# as a characteristic of the Device, and can help with differnt predictions
# Anyways this wouldn't be on this exercise

df['max_performance [MB/min]'] = 0

# Asign 6000 MB/min to file devices, like BASt_B2D_Writer0, BASt_B2D_Writer1, BASt_B2D_Writer2
df.loc[df['Device'].isin([
    'BASt_B2D_Writer0', 'BASt_B2D_Writer1', 'BASt_B2D_Writer2'
]), 'max_performance [MB/min]'] = 6000

# Asign 9600 MB/min to LTO6 devices like HP:Ultrium 6-SCSI_1_backup, ..., HP:Ulitrium 6-SCSI_6_backup
df.loc[df['Device'].isin([
    'HP:Ultrium 6-SCSI_1_backup', 'HP:Ultrium 6-SCSI_2_backup', 'HP:Ultrium 6-SCSI_3_backup',
    'HP:Ultrium 6-SCSI_4_backup', 'HP:Ultrium 6-SCSI_5_backup', 'HP:Ultrium 6-SCSI_6_backup'
]), 'max_performance [MB/min]'] = 9600

# Asign 21600 MB/min to LT8 device HP:Ulitrium 8-SCSI_1_backup, ..., LTO_Drive_12
df.loc[df['Device'].isin([
    'HP:Ultrum 8-SCSI_1_backup', 'LTO_Drive_01', 'LTO_Drive_02', 'LTO_Drive_03', 'LTO_Drive_04',
    'LTO_Drive_05', 'LTO_Drive_06', 'LTO_Drive_07', 'LTO_Drive_08', 'LTO_Drive_09',
    'LTO_Drive_10', 'LTO_Drive_11', 'LTO_Drive_12'
]), 'max_performance [MB/min]'] = 21600

df['max_capacity [TB]'] = 0

# Asign 800 MB/ to file devices, like BASt_B2D_Writer0, BASt_B2D_Writer1, BASt_B2D_Writer2
df.loc[df['Device'].isin([
    'BASt_B2D_Writer0', 'BASt_B2D_Writer1', 'BASt_B2D_Writer2'
]), 'max_capacity [TB]'] = 8

# Asign 2560 MB/min to LTO6 devices like HP:Ultrium 6-SCSI_1_backup, ..., HP:Ulitrium 6-SCSI_6_backup
df.loc[df['Device'].isin([
    'HP:Ultrium 6-SCSI_1_backup', 'HP:Ultrium 6-SCSI_2_backup', 'HP:Ultrium 6-SCSI_3_backup',
    'HP:Ultrium 6-SCSI_4_backup', 'HP:Ultrium 6-SCSI_5_backup', 'HP:Ultrium 6-SCSI_6_backup'
]), 'max_capacity [TB]'] = 6

# Asign 21600 MB/min to LT8 device HP:Ulitrium 8-SCSI_1_backup, ..., LTO_Drive_12
df.loc[df['Device'].isin([
    'HP:Ultrium 8-SCSI_1_backup', 'LTO_Drive_01', 'LTO_Drive_02', 'LTO_Drive_03', 'LTO_Drive_04',
    'LTO_Drive_05', 'LTO_Drive_06', 'LTO_Drive_07', 'LTO_Drive_08', 'LTO_Drive_09',
    'LTO_Drive_10', 'LTO_Drive_11', 'LTO_Drive_12'
]), 'max_capacity [TB]'] = 30

#
# Sessions with errors or warnings can also have incorrect or missing values in
# some fields, this can produce errors at the time of generating the graph, so
# I'm going to use only sessions with Status Completed
df = df[df['Status'] == 'Completed']
# df = df[df['Mode'] == 'full']

df.info()

plot_histograms(df)

df = normalize_numeric_features(df)

#df.head(50).to_csv('df_backup50.csv', index=False)

# Seleccionar solo las columnas numéricas
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Calcular la matriz de correlación
correlation_matrix = numeric_df.corr()

# Visualizar la matriz de correlación
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Matriz de Correlación')
plt.show()

#
# Looking into the correlation matrix there no variable with an strong correlation
# with the percentaje of success, so traditional ML approach wouldn't work
#

#
# Crear un nuevo grafo con las propiedades que vamos a necesitar para el analisis
#

# Encode categorical features: Mode and Mountpoint
le_mode = LabelEncoder()
df['Mode'] = le_mode.fit_transform(df['Mode'])

le_mountpoint = LabelEncoder()
df['Mountpoint'] = le_mountpoint.fit_transform(df['Mountpoint'])

# Create embedding for Mode
num_modes = len(le_mode.classes_)
embedding_dim_mode = 4
mode_embedding = nn.Embedding(num_modes, embedding_dim_mode)

# Create embedding for Mountpoint
num_mountpoints = len(le_mountpoint.classes_)
embedding_dim_mountpoint = 8
mountpoint_embedding = nn.Embedding(num_mountpoints, embedding_dim_mountpoint)

# Applying embeddings to data
clients = df[['Client', 'RPO', 'RTO', 'Data Written [GB]','Total # Files', '# Objects','# Completed DA','Success']].drop_duplicates().reset_index(drop=True)
devices = df[['Device', 'max_performance [MB/min]']].drop_duplicates().reset_index(drop=True)

# Creating Client Node features (without Mountpoint)
client_embedded_features = torch.tensor(clients[['RPO', 'RTO','Data Written [GB]','Total # Files', '# Objects','# Completed DA','Success' ]].values, dtype=torch.float)

# Add more features or a dummy feature to match the dimension of client features
device_features = torch.cat([
    torch.tensor(devices[['max_performance [MB/min]']].values, dtype=torch.float),
    torch.zeros(len(devices), client_embedded_features.shape[1]-1)  # Add dummy features
], dim=1)

# Creating dictionaries to map names to indices of the nodes
client_index = {name: i for i, name in enumerate(clients['Client'])}
device_index = {name: i + len(clients) for i, name in enumerate(devices['Device'])}

# Edges and edge features
edges = []
edge_features = []

for _, row in df.iterrows():
    client_id = client_index[row['Client']]
    device_id = device_index[row['Device']]

    edges.append([client_id, device_id])

    # Create embedding for Mode
    mode_embedded = mode_embedding(torch.tensor(row['Mode']))

    # Create embedding for Mountpoint (now as an edge attribute)
    mountpoint_embedded = mountpoint_embedding(torch.tensor(row['Mountpoint']))

    # Convert datetime objects to numeric timestamps (seconds since epoch)
    start_time_seconds = row['Start Time'].timestamp()
    end_time_seconds = row['End Time'].timestamp()

    # Create a list of numeric edge attributes
    other_edge_attr_values = [
        start_time_seconds,
        end_time_seconds,
        row['Duration_seconds'],
        row['Size [kB]'],
        row['# Files'],
        row['Performance [MB/min]']
    ]

    # Convert the list to a PyTorch tensor
    other_edge_attr = torch.tensor(other_edge_attr_values, dtype=torch.float)

    # Concatenate all edge features, including Mode and Mountpoint embeddings
    edge_features.append(torch.cat([mode_embedded, mountpoint_embedded, other_edge_attr], dim=0))

# Converting to tensors
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
edge_attr = torch.stack(edge_features)

# Creating the Data object in PyTorch Geometric
data = Data(x=torch.cat([client_embedded_features, device_features], dim=0),
            edge_index=edge_index,
            edge_attr=edge_attr)

print(data)

#
# Divide data in Test, Train and Validation sets
#
# Number of nodes
num_nodes = data.num_nodes

# Creating index
train_indices, test_indices = train_test_split(range(num_nodes), test_size=0.2, random_state=42)
train_indices, val_indices = train_test_split(train_indices, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2

# Starting Mask
train_mask = torch.zeros(num_nodes, dtype=torch.bool)
val_mask = torch.zeros(num_nodes, dtype=torch.bool)
test_mask = torch.zeros(num_nodes, dtype=torch.bool)

# Asigning Masks to each node
train_mask[train_indices] = True
val_mask[val_indices] = True
test_mask[test_indices] = True

# Adding masks to the data object
data.train_mask = train_mask
data.val_mask = val_mask
data.test_mask = test_mask

# Cheching the masks has ben created correctly
print(f'Train mask: {train_mask.sum()} nodes')
print(f'Validation mask: {val_mask.sum()} nodes')
print(f'Test mask: {test_mask.sum()} nodes')

# Paso 1: Preparar los datos
# La característica 'Failure' será la que queremos predecir.
y = data.x[:, -1]  # Sabemos que 'Success' es la última característica en data.x
# Normalize y to be between 0 and 1
# y = (y - y.min()) / (y.max() - y.min()) # Normalize target variable
data.y = y

# Eliminar 'Success' de las características de entrada
data.x = data.x[:, :-1]

# Paso 2: Definir un modelo GCN
class DeepGCN(torch.nn.Module):
    def __init__(self, num_node_features, hidden_channels, dropout=0.5):
        super(DeepGCN, self).__init__()

        self.conv1 = GCNConv(num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.conv4 = GCNConv(hidden_channels, hidden_channels)
        self.conv5 = GCNConv(hidden_channels, 1)  # Output layer

        self.dropout = dropout

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = F.relu(self.conv2(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = F.relu(self.conv3(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = F.relu(self.conv4(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv5(x, edge_index)  # No activation in the output layer for regression

        return torch.sigmoid(x)  # For binary classification (adjust if needed)

# Instancing the model
model = DeepGCN(num_node_features=data.x.shape[1], hidden_channels=16)

# Training the model
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.BCELoss()

def train():
    model.train()
    optimizer.zero_grad()
    out = model(data)
    loss = criterion(out.flatten(), data.y)
    loss.backward(retain_graph=True)  # retain_graph=True, needed to reuse the graph
    optimizer.step()
    return loss.item()

def validate():
    model.eval()
    with torch.no_grad():
        out = model(data)
        val_loss = criterion(out[data.val_mask].flatten(), data.y[data.val_mask])
    return val_loss.item()

def test():
    model.eval()
    with torch.no_grad():
        out = model(data)
        test_loss = criterion(out[data.test_mask].flatten(), data.y[data.test_mask])
        accuracy = (out[data.test_mask].flatten().round() == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()
    return test_loss.item(), accuracy

# Train by epoch
for epoch in range(500):  #
    loss = train()
    val_loss = validate()
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')

# Evaluate test
test_loss, test_accuracy = test()
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import torch
from torch_geometric.data import Data

def calculate_metrics(pred, target):
    # Convert predictions and objetives to numpy
    pred = pred.flatten().cpu().numpy()
    target = target.cpu().numpy()

    # Calculate metrics
    mse = mean_squared_error(target, pred)
    rmse = mean_squared_error(target, pred, squared=False)  # RMSE es la raíz cuadrada del MSE
    mae = mean_absolute_error(target, pred)
    r2 = r2_score(target, pred)

    return mse, rmse, mae, r2

def evaluate(data, mask):
    model.eval()
    with torch.no_grad():
        out = model(data)
        pred = out[mask]
        target = data.y[mask]
        return calculate_metrics(pred, target)

# Evaluate metrics
train_metrics = evaluate(data, data.train_mask)
val_metrics = evaluate(data, data.val_mask)
test_metrics = evaluate(data, data.test_mask)

print(f'Train Metrics: MSE={train_metrics[0]:.4f}, RMSE={train_metrics[1]:.4f}, MAE={train_metrics[2]:.4f}, R2={train_metrics[3]:.4f}')
print(f'Validation Metrics: MSE={val_metrics[0]:.4f}, RMSE={val_metrics[1]:.4f}, MAE={val_metrics[2]:.4f}, R2={val_metrics[3]:.4f}')
print(f'Test Metrics: MSE={test_metrics[0]:.4f}, RMSE={test_metrics[1]:.4f}, MAE={test_metrics[2]:.4f}, R2={test_metrics[3]:.4f}')

# First Prediction: Client with the biggest risk of failing
model.eval()
with torch.no_grad():
    out = model(data).flatten()  # Node Predictions

# Extract the predictions corresponding to the nodes of type 'Client'
client_predictions = out[:len(clients)]  # Extrayendo las predicciones para los nodos 'Client'

# Calculate the probability of failure as 1 - Success
failure_probabilities = 1 - client_predictions.cpu().numpy()

# Identify the client with the highest risk of failure
max_risk_index = failure_probabilities.argmax()  # Ahora usamos argmax para obtener la mayor probabilidad de fallo

# Get the name of the client with the highest risk
client_at_risk = clients.iloc[max_risk_index]['Client']

print(f'El cliente con mayor riesgo de fallo es: {client_at_risk} con una probabilidad de fallo de {failure_probabilities[max_risk_index]:.4f}')

# Set the model to evaluation mode
model.eval()
with torch.no_grad():
    #  Assuming 'data' is defined, compute 'out'
    out = model(data).flatten()  # Node Predictions

    # Extract the predictions corresponding to the nodes of type 'Client'
    client_predictions = out[:len(clients)]

    # Calculate the probability of failure as 1 - Success
    failure_probabilities = 1 - client_predictions.cpu().numpy()

    # Get the indices of the top 10 clients with the highest failure probabilities
    top_10_risk_indices = np.argsort(failure_probabilities)[-10:][::-1]

    # Get the names of the top 10 clients at risk
    top_10_clients_at_risk = clients.iloc[top_10_risk_indices]['Client']

    # Print the top 10 clients and their failure probabilities
    print("Top 10 clients with highest risk of failure:")
    for i, (client, probability) in enumerate(zip(top_10_clients_at_risk, failure_probabilities[top_10_risk_indices])):
        print(f'Rank {i+1}: Client {client} with a probability of failure of {probability:.4f}')

# Create a bar plot
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
plt.bar(top_10_clients_at_risk, failure_probabilities[top_10_risk_indices])
plt.xlabel('Client')
plt.ylabel('Probability of Failure')
plt.title('Top 10 Clients with Highest Risk of Failure')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

import matplotlib.pyplot as plt

# Filtrar los clientes con el valor máximo de RTO
max_rto = clients['RTO'].max()
clients_with_max_rto = clients[clients['RTO'] == max_rto]

# Ordenar por probabilidad de éxito (Success) y seleccionar el Top 10
top_10_clients_with_max_rto = clients_with_max_rto.sort_values(by='Success', ascending=False).head(10)

# Imprimir los clientes con su probabilidad de éxito
print("Top 10 clientes con RTO máximo y su probabilidad de éxito del backup:")
print(top_10_clients_with_max_rto[['Client', 'Success']])

# Crear el gráfico de barras horizontales para mostrar la probabilidad de éxito (Success)
plt.figure(figsize=(10, 8))
plt.barh(top_10_clients_with_max_rto['Client'], top_10_clients_with_max_rto['Success'], color='skyblue')
plt.xlabel('Probabilidad de Éxito del Backup')
plt.ylabel('Cliente')
plt.title('Top 10 Clientes con RTO Máximo y su Probabilidad de Éxito del Backup')
plt.gca().invert_yaxis()  # Para que el cliente con mayor probabilidad esté en la parte superior
plt.show()